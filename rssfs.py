# -*- coding: utf-8 -*-
"""RSSFs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W7Jbyx-CK0LJK5jD9qsPseK8hJU0XoJt
"""

#!pip install python_speech_features
#!pip install tensorflow==2.0.0-rc1

import os
from scipy.io import wavfile
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from keras.layers import Conv2D, MaxPool2D, Flatten, LSTM
from keras.layers import Dropout, Dense, TimeDistributed
from keras.models import Sequential
from keras.utils import to_categorical
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
from python_speech_features import mfcc, logfbank
import librosa as librosa
import pickle
from keras.callbacks import ModelCheckpoint
import tensorflow as tf
import pickle
import os
from keras.models import load_model

def plot_signals(signals):
    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,
                             sharey=True, figsize=(20,5))
    fig.suptitle('Time Series', size=16)
    i = 0
    for x in range(2):
        for y in range(5):
            axes[x,y].set_title(list(signals.keys())[i])
            axes[x,y].plot(list(signals.values())[i])
            axes[x,y].get_xaxis().set_visible(False)
            axes[x,y].get_yaxis().set_visible(False)
            i += 1

def plot_fft(fft):
    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,
                             sharey=True, figsize=(20,5))
    fig.suptitle('Fourier Transforms', size=16)
    i = 0
    for x in range(2):
        for y in range(5):
            data = list(fft.values())[i]
            Y, freq = data[0], data[1]
            axes[x,y].set_title(list(fft.keys())[i])
            axes[x,y].plot(freq, Y)
            axes[x,y].get_xaxis().set_visible(False)
            axes[x,y].get_yaxis().set_visible(False)
            i += 1

def plot_fbank(fbank):
    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,
                             sharey=True, figsize=(20,5))
    fig.suptitle('Filter Bank Coefficients', size=16)
    i = 0
    for x in range(2):
        for y in range(5):
            axes[x,y].set_title(list(fbank.keys())[i])
            axes[x,y].imshow(list(fbank.values())[i],
                    cmap='hot', interpolation='nearest')
            axes[x,y].get_xaxis().set_visible(False)
            axes[x,y].get_yaxis().set_visible(False)
            i += 1

def plot_mfccs(mfccs):
    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,
                             sharey=True, figsize=(20,5))
    fig.suptitle('Mel Frequency Cepstrum Coefficients', size=16)
    i = 0
    for x in range(2):
        for y in range(5):
            axes[x,y].set_title(list(mfccs.keys())[i])
            axes[x,y].imshow(list(mfccs.values())[i],
                    cmap='hot', interpolation='nearest')
            axes[x,y].get_xaxis().set_visible(False)
            axes[x,y].get_yaxis().set_visible(False)
            i += 1

def calc_fft(y, rate):
  n = len(y)
  freq = np.fft.rfftfreq(n, d=1/rate)
  Y = abs(np.fft.rfft(y)/n)
  return (Y, freq)

def envelope(y, rate, threshold):
  mask = []
  y = pd.Series(y).apply(np.abs)
  y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()
  for mean in y_mean:
    if mean > threshold:
      mask.append(True)
    else:
      mask.append(False)
  return mask

def check_data():
  if os.path.isfile(config.p_path):
    print('Loading existing data for {} model'.format(config.mode))
    with open(config.p_path, 'rb') as handle:
      tmp = pickle.load(handle)
      return tmp
  else:
    return None

def build_rand_feat():
  tmp = check_data()
  if tmp:
     return tmp.data[0], tmp.data[1]
  X = []
  y = []
  _min, _max = float('inf'), -float('inf') # 
  for _ in tqdm(range(n_samples)): # just to show the progress
    rand_class = np.random.choice(class_dist2.index, p=prob_dist) # escolhemos uma classe aleatoria (violao, violoncelo e etc)
    file = np.random.choice(df2[df2.label == rand_class].index) # pegamos um arquivo de tal classe
    rate, wav = wavfile.read('/content/drive/My Drive/RSSFs/clean_2/'+file) # pega o arquivo
    label = df2.at[file, 'label'] # pegamos o label
    rand_index = np.random.randint(0, wav.shape[0]-config.step)
    sample = wav[ :rand_index+config.step]
    X_sample = mfcc(sample, rate, numcep=config.nfeat, nfilt= config.nfilt, nfft=config.nfft)
    _min = min(np.amin(X_sample), _min)
    _max = max(np.amax(X_sample), _max)
    X.append(X_sample)
    y.append(classes2.index(label))
  config.min = _min 
  config.max = _max 
  X, y = np.array(X), np.array(y)
  X = (X - _min) / (_max - _min)
  X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)
  y = to_categorical(y, num_classes=10)
  config.data = (X, y)

  with open(config.p_path, 'wb') as handle:
    pickle.dump(config, handle, protocol=2)

  return X, y

def build_predictions(audio_dir):
  y_true = []
  y_pred = []
  fn_prob = {}

  print('Extraindo features do audio')
  for fn in tqdm(os.listdir(audio_dir)):
    rate, wav = wavfile.read(os.path.join(audio_dir, fn))
    label = fn2class[fn]
    c = classes.index(label)
    y_prob = []

    for i in range(0, wav.shape[0]-config.step, config.step):
      sample = wav[i:i+config.step]
      x = mfcc(sample, rate, numcep=config.nfeat,
               nfilt=config.nfilt, nfft=config.nfft)
      x = (x - config.min)/ (config.max - config.min)

      x = x.reshape(1, x.shape[0], x.shape[1], 1)

      y_hat = model.predit(x)
      y_prob.append(y_hat)
      y_pred.append(np.argmax(y_hat))
      y_true.append(c)

    fn_prob[fn] = np.mean(y_prob, axis=0).flatten()

  return y_true, y_pred, fn_prob

class Config:
  def __init__(self, mode='conv',nfilt=26, nfeat=13, nfft=1103, rate=44100):
    self.mode = mode
    self.nfilt = nfilt
    self.nfeat = nfeat
    self.rate = rate
    self.step = int(rate/10)
    self.nfft = nfft
    self.model_path = os.path.join('/models', mode + '.model')
    self.p_path = os.path.join('/pickles', mode + '.p')

def get_conv_model():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', strides=(1,1), padding= 'same', input_shape=input_shape)) 
  model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', strides=(1,1), padding= 'same'))
  model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', strides=(1,1), padding= 'same'))
  model.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1,1), padding= 'same'))
  model.add(tf.keras.layers.MaxPool2D((2,2)))

# Convolução feita


  model.add(tf.keras.layers.Dropout(0.5))
  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(128, activation='relu'))
  model.add(tf.keras.layers.Dense(64, activation='relu'))
  model.add(tf.keras.layers.Dense(32, activation='relu'))
  model.add(tf.keras.layers.Dense(10, activation='softmax'))
  model.summary()
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
  return model

###############################################################################################################################################################################################################################################################
###############################################################################################################################################################################################################################################################

df = pd.read_csv('./instruments.csv')
df.set_index('fname', inplace=True)

for f in df.fname:
  rate, signal = wavfile.read('/wavfiles/'+f)
  df.at[f,'length'] = signal.shape[0]/rate

classes = list(np.unique(df.label))
class_dist = df.groupby(['label'])['length'].mean()

ax = plt.subplot()
ax.set_title('Class Distribution', y=1.08)
ax.pie(class_dist, labels=class_dist.index, autopct='%1.1f%%', startangle=90)
ax.axis('equal')
plt.show()



for c in classes:
  print(df[df.label == c].iloc[0,0])

for c in classes:
  wav_file = df[df.label == c].iloc[0,0]
  signal, rate = librosa.load('/clean/'+wav_file, sr=44100)
  mask = envelope(signal, rate, 0.0005)
  signal = signal[mask]
  signals[c] = signal
  fft[c] = calc_fft(signal, rate)
  bank = logfbank(signal[:rate], rate, nfilt=26, nfft=1103).T
  fbank[c] = bank
  mel = mfcc(signal[:rate], rate, numcep=13, nfilt=26, nfft=1103).T
  mfccs[c] = mel


plot_signals(signals)
plt.show()

plot_fft(fft)
plt.show()

plot_fbank(fbank)
plt.show()

plot_mfccs(mfccs)
plt.show()



if len(os.listdir('/clean')) == 0:
  for f in tqdm(df.fname):
    signal, rate = librosa.load('/wavfiles/'+f, sr =44100)
    mask = envelope(signal, rate, 0.0005)
    wavfile.write(filename='/clean/'+f,rate=rate, data=signal[mask])



df2 = pd.read_csv('./instruments.csv')
df2.set_index('fname', inplace=True)

for f in df2.index:
  rate, signal = wavfile.read('/clean/'+f)
  df2.at[f,'length'] = signal.shape[0]/rate

classes2 = list(np.unique(df2.label))
class_dist2 = df2.groupby(['label'])['length'].mean()

n_samples = 2 * int(df2['length'].sum()/0.1)
prob_dist = class_dist2 / class_dist2.sum()
choices = np.random.choice(class_dist2.index, p=prob_dist)

fig, ax = plt.subplots()
ax.set_title('Class Distribution', y=1.08)
ax.pie(class_dist2, labels=class_dist2.index, autopct='%1.1f%%', shadow=False, startangle=90)
ax.axis('equal')
plt.show()

config = Config(mode='conv')

X, y = build_rand_feat()

y_flat = np.argmax(y, axis=1)
input_shape = (X.shape[1], X.shape[2], 1)

model = get_conv_model()

class_weight = compute_class_weight('balanced', np.unique(y_flat), y_flat)

checkpoint = ModelCheckpoint(config.model_path, monitor='val_acc', verbose=1, mode='max', save_best_only=True, save_weights_only=False, period=1)

history = model.fit(X, y, epochs = 100, batch_size=32, shuffle=True, class_weight = class_weight, validation_split=0.1, callbacks=[checkpoint])

model.save(config.model_path)

# increase the size of the graphs. The default size is (6,4).
plt.rcParams["figure.figsize"] = (20,10)

# graph the loss, the model above is configure to use "mean squared error" as the loss function
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'g.', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

print(plt.rcParams["figure.figsize"])



loaded_model = pickle.load(open("/content/drive/My Drive/RSSFs/models/conv.model", 'rb'))

# Convert the model to the TensorFlow Lite format without quantization
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model to disk
open("/sound_model.tflite", "wb").write(tflite_model)
  
basic_model_size = os.path.getsize("/sound_model.tflite")
print("Model is %d bytes" % basic_model_size)

#!apt-get -qq install xxd

#!echo "const unsigned char model[] = {" > /content/model.h
#!cat gesture_model.tflite | xxd -i      >> /content/model.h
#!echo "};"                              >> /content/model.h

#model_h_size = os.path.getsize("model.h")
#print(f"Header file, model.h, is {model_h_size:,} bytes.")
#print("\nOpen the side panel (refresh if needed). Double click model.h to download the file.")

df = pd.read_csv('./instruments.csv')
classes = list(np.unique(df.label))
fn2class = dict(zip(df.fname, df.label))

print(fn2class)

p_path = os.path.join('/pickles', 'conv.p')

with open(p_path, 'rb') as handle:
  config = pickle.load(handle)

print('MIN: ',config.min)
print('MAX: ',config.max)
print('STEP: ',config.step)
print('NFILT: ',config.nfilt)
print('NFEAT: ',config.nfeat)
print('NFFT: ',config.nfft)

model = load_model(config.model_path)

y_true, y_pred, fn_prob = build_predictions('clean')
acc_score = accuracy_score(y_true=y_true, y_pred=y_pred)

y_probs = []
for i, row in df.iterrows():
   y_prob = fn_prob[row.fname]
   y_probs.append(y_prob)
   for c, p in zip(classes, y_prob):
     df.at[i, c] = p

 y_pred = [classes[np.argmax(y)] for y in y_probs]
 df['y_pred'] = y_pred

 df.to_csv('/instruments.csv', index=false)